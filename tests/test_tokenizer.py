"""
Test script for the Advanced Multi-Modal Tokenizer
"""

import os
import sys
# Add parent directory to path so we can import from tokenization module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tokenization.multimodal_tokenizer import MultiModalTokenizer, TokenizationConfig, ModalityType

def test_multimodal_tokenizer():
    """Test the advanced multi-modal tokenizer with various data types."""
    
    print("üöÄ TESTING ADVANCED MULTI-MODAL TOKENIZER")
    print("=" * 60)
    
    # Initialize tokenizer
    config = TokenizationConfig(
        text_vocab_size=8000,
        max_text_length=512,
        max_sequence_length=2048
    )
    
    tokenizer = MultiModalTokenizer(config)
    
    # Sample texts for training the text tokenizer
    sample_texts = [
        "This is a comprehensive test of the RT-DLM AGI tokenizer system.",
        "The tokenizer can handle text, images, audio, video, PDFs, XML, ZIP files, and binary data.",
        "Multi-modal AI systems require unified tokenization across all data types.",
        "Artificial General Intelligence needs to understand diverse input modalities.",
        "The RT-DLM model processes information like humans do - across multiple senses.",
        "Advanced tokenization enables seamless cross-modal reasoning and understanding.",
        "From text documents to multimedia files, everything becomes tokens for the AI.",
        "This unified representation allows for true multi-modal intelligence.",
        "The future of AI is in understanding all forms of human communication and data.",
        "RT-DLM tokenizer: bridging the gap between raw data and AI understanding."
    ]
    
    print("üîß Training text tokenizer...")
    try:
        tokenizer.train_text_tokenizer(sample_texts, "tokenizers/agi_text_model")
        print("‚úÖ Text tokenizer trained successfully!")
    except Exception as e:
        print(f"‚ö†Ô∏è Text tokenizer training failed: {e}")
        print("üîÑ Continuing with fallback character-level tokenization...")
    
    print("\n" + "=" * 60)
    print("üß™ TESTING DIFFERENT DATA MODALITIES")
    print("=" * 60)
    
    # Test cases for different modalities
    test_cases = [
        # Text data
        {
            "name": "üìù Plain Text",
            "data": "Hello, this is a test of the AGI tokenizer! It can handle various text formats.",
            "modality": ModalityType.TEXT
        },
        {
            "name": "üíª Code",
            "data": "def hello_world():\n    print('Hello, AGI!')\n    return 42",
            "modality": ModalityType.CODE
        },
        {
            "name": "üìä JSON Data",
            "data": '{"name": "RT-DLM", "type": "AGI", "capabilities": ["text", "image", "audio", "video"]}',
            "modality": ModalityType.JSON
        },
        {
            "name": "üåê XML Data",
            "data": "<ai><name>RT-DLM</name><type>AGI</type><version>2.0</version></ai>",
            "modality": ModalityType.XML
        },
        # Binary-like data
        {
            "name": "üî¢ Binary Data",
            "data": b'\x89\x50\x4e\x47\x0d\x0a\x1a\x0a\x00\x00\x00\x0d\x49\x48\x44\x52',
            "modality": ModalityType.BINARY
        }
    ]
    
    # Process each test case
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. {test_case['name']}")
        print("-" * 40)
        
        data = test_case['data']
        expected_modality = test_case['modality']
        
        # Show input data
        if isinstance(data, bytes):
            print(f"Input: {data.hex()[:50]}{'...' if len(data.hex()) > 50 else ''}")
        else:
            print(f"Input: {str(data)[:100]}{'...' if len(str(data)) > 100 else ''}")
        
        # Auto-detect modality
        detected_modality = tokenizer.detect_modality(data)
        print(f"Detected Modality: {detected_modality.value}")
        print(f"Expected Modality: {expected_modality.value}")
        
        # Tokenize
        try:
            tokens = tokenizer.tokenize(data, expected_modality)
            print(f"‚úÖ Tokenization successful!")
            print(f"Token count: {len(tokens)}")
            print(f"First 10 tokens: {tokens[:10]}")
            print(f"Last 10 tokens: {tokens[-10:]}")
            
            # Try to detokenize text if it's a text modality
            if expected_modality == ModalityType.TEXT:
                # Extract text tokens (between TEXT_START and TEXT_END)
                text_tokens = []
                in_text = False
                for token in tokens:
                    if token == tokenizer.modality_tokens.get("TEXT_START"):
                        in_text = True
                    elif token == tokenizer.modality_tokens.get("TEXT_END"):
                        in_text = False
                    elif in_text:
                        text_tokens.append(token)
                
                if text_tokens:
                    try:
                        detokenized = tokenizer.detokenize_text(text_tokens)
                        print(f"Detokenized: {detokenized[:100]}{'...' if len(detokenized) > 100 else ''}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Detokenization failed: {e}")
        
        except Exception as e:
            print(f"‚ùå Tokenization failed: {e}")
    
    print("\n" + "=" * 60)
    print("üìà TOKENIZER STATISTICS")
    print("=" * 60)
    
    print(f"Total vocabulary size: {tokenizer.get_vocab_size():,}")
    print(f"Text vocabulary size: {config.text_vocab_size:,}")
    print(f"Image vocabulary size: {config.image_vocab_size:,}")
    print(f"Audio vocabulary size: {config.audio_vocab_size:,}")
    print(f"Max sequence length: {config.max_sequence_length:,}")
    
    print("\nüéØ MODALITY TOKEN MAPPING")
    print("-" * 30)
    for name, token_id in tokenizer.modality_tokens.items():
        print(f"{name}: {token_id}")
    
    print("\n" + "=" * 60)
    print("üéâ MULTI-MODAL TOKENIZER TEST COMPLETE!")
    print("=" * 60)
    
    print("\nüìã SUMMARY:")
    print("‚úÖ Text tokenization with SentencePiece/character fallback")
    print("‚úÖ JSON/XML structured data tokenization")
    print("‚úÖ Code tokenization with syntax awareness")
    print("‚úÖ Binary data tokenization")
    print("‚úÖ Modality detection and token mapping")
    print("‚úÖ Unified token sequence with modality markers")
    
    print("\nüöÄ READY FOR AGI TRAINING!")
    print("The tokenizer can now handle ANY data type for your RT-DLM AGI system.")
    
    return tokenizer

def create_sample_files_for_testing():
    """Create sample files for testing different modalities."""
    
    os.makedirs("test_data", exist_ok=True)
    
    # Create sample text file
    with open("test_data/sample.txt", "w") as f:
        f.write("This is a sample text file for testing the AGI tokenizer.")
    
    # Create sample JSON file
    with open("test_data/sample.json", "w") as f:
        f.write('{"model": "RT-DLM", "version": "2.0", "capabilities": ["AGI", "multimodal"]}')
    
    # Create sample XML file
    with open("test_data/sample.xml", "w") as f:
        f.write("""<?xml version="1.0"?>
<agi>
    <name>RT-DLM</name>
    <type>Advanced AGI</type>
    <features>
        <feature>Multi-modal processing</feature>
        <feature>Quantum enhancement</feature>
        <feature>Consciousness simulation</feature>
    </features>
</agi>""")
    
    # Create sample Python code file
    with open("test_data/sample.py", "w") as f:
        f.write("""# RT-DLM AGI Sample Code
def process_multimodal_input(text, image, audio, video):
    \"\"\"Process multi-modal input through AGI system.\"\"\"
    
    # Tokenize all modalities
    text_tokens = tokenizer.tokenize(text, ModalityType.TEXT)
    image_tokens = tokenizer.tokenize(image, ModalityType.IMAGE)
    audio_tokens = tokenizer.tokenize(audio, ModalityType.AUDIO)
    video_tokens = tokenizer.tokenize(video, ModalityType.VIDEO)
    
    # Combine into unified sequence
    unified_tokens = text_tokens + image_tokens + audio_tokens + video_tokens
    
    # Process through AGI model
    agi_output = model.forward(unified_tokens)
    
    return agi_output
""")
    
    print("üìÅ Sample test files created in 'test_data/' directory")

if __name__ == "__main__":
    # Create sample files for testing
    create_sample_files_for_testing()
    
    # Run the main test
    tokenizer = test_multimodal_tokenizer()
    
    print("\nüî¨ TESTING WITH SAMPLE FILES")
    print("=" * 40)
    
    # Test with actual files
    file_tests = [
        ("test_data/sample.txt", ModalityType.TEXT),
        ("test_data/sample.json", ModalityType.JSON),
        ("test_data/sample.xml", ModalityType.XML),
        ("test_data/sample.py", ModalityType.CODE)
    ]
    
    for file_path, modality in file_tests:
        if os.path.exists(file_path):
            print(f"\nüìÑ Testing {file_path}")
            try:
                tokens = tokenizer.tokenize(file_path, modality)
                print(f"‚úÖ File tokenized: {len(tokens)} tokens")
                print(f"Sample tokens: {tokens[:5]}...{tokens[-5:]}")
            except Exception as e:
                print(f"‚ùå Error: {e}")
    
    print("\nüéØ All tests completed! Your AGI tokenizer is ready!")
